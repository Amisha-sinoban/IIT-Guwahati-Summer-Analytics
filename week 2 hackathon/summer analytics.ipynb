{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e3bad7-66b2-4563-932a-c454ec91c039",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m train_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mHP\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDocuments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mSA\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mhacktrain.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m test_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mHP\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDocuments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mSA\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mhacktest.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_df = \u001b[43mpd\u001b[49m.read_csv(train_path)\n\u001b[32m      4\u001b[39m test_df = pd.read_csv(test_path)\n\u001b[32m      5\u001b[39m test_ids = test_df[\u001b[33m\"\u001b[39m\u001b[33mID\u001b[39m\u001b[33m\"\u001b[39m].copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "train_path = r\"C:\\Users\\HP\\Documents\\SA\\hacktrain.csv\"\n",
    "test_path = r\"C:\\Users\\HP\\Documents\\SA\\hacktest.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "test_ids = test_df[\"ID\"].copy()\n",
    "print(\"okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59ba6f-9732-4489-a76e-2bd7534993a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loading section completed. Variables 'train_df', 'test_df', 'test_ids', and 'ndvi_cols' are now available.\n"
     ]
    }
   ],
   "source": [
    "ndvi_cols = [col for col in train_df.columns if '_N' in col]\n",
    "\n",
    "print(\"\\nData loading section completed. Variables 'train_df', 'test_df', 'test_ids', and 'ndvi_cols' are now available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c73b4-b884-4446-bdb2-05ec84330447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Preprocessing Training Data...\n",
      "Dropped 'Unnamed: 0' column from train_df.\n",
      "Original training features shape (X_train_raw): (8000, 27)\n",
      "Original training target shape (y_train_raw): (8000,)\n",
      "Training data: Missing values imputed.\n",
      "Training data: Features scaled.\n",
      "\n",
      "Preprocessing for Training Data completed.\n",
      "Variables 'X_train_raw', 'y_train_raw', 'imputer', 'scaler', 'X_train_imputed', and 'X_train_scaled' are now available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming train_df and ndvi_cols are already defined from Step 1.\n",
    "# If you are running this as a separate block, ensure these variables are available.\n",
    "\n",
    "\n",
    "# --- Step 2: Preprocessing for Training Data ---\n",
    "print(\"\\nStep 2: Preprocessing Training Data...\")\n",
    "\n",
    "# Drop 'Unnamed: 0' column if it exists (often created during CSV export/import)\n",
    "if \"Unnamed: 0\" in train_df.columns:\n",
    "    train_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    print(\"Dropped 'Unnamed: 0' column from train_df.\")\n",
    "\n",
    "# Separate features (X) and target (y) for training\n",
    "X_train_raw = train_df[ndvi_cols].copy()\n",
    "y_train_raw = train_df[\"class\"].copy() # This is your target variable\n",
    "\n",
    "print(f\"Original training features shape (X_train_raw): {X_train_raw.shape}\")\n",
    "print(f\"Original training target shape (y_train_raw): {y_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# Handle Missing Values (Imputation)\n",
    "# Using median strategy as it's robust to outliers\n",
    "imputer = SimpleImputer(strategy=\"median\") # Initialize the imputer\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_raw), columns=ndvi_cols)\n",
    "print(\"Training data: Missing values imputed.\")\n",
    "\n",
    "# Scale Features (Standardization)\n",
    "# Scales features to have a mean of 0 and a standard deviation of 1\n",
    "scaler = StandardScaler() # Initialize the scaler\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=ndvi_cols)\n",
    "print(\"Training data: Features scaled.\")\n",
    "\n",
    "print(\"\\nPreprocessing for Training Data completed.\")\n",
    "print(\"Variables 'X_train_raw', 'y_train_raw', 'imputer', 'scaler', 'X_train_imputed', and 'X_train_scaled' are now available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b17e8-597c-4a96-a804-8f46c21f773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Feature Engineering for Training Data...\n",
      "Training data: New features created.\n",
      "Engineered training features shape: (8000, 8)\n",
      "   ndvi_mean  ndvi_std  ndvi_min  ndvi_max  ndvi_range  ndvi_first  ndvi_last  \\\n",
      "0  -1.483386  1.106149 -4.810609  0.284937    5.095546   -2.403797  -0.874448   \n",
      "1  -1.614397  0.986777 -4.837167  0.213515    5.050682   -2.405323  -0.836678   \n",
      "2  -1.382754  1.204176 -4.810100  0.426859    5.236959   -2.667544  -1.618977   \n",
      "3  -1.547693  1.055514 -4.807409  0.214805    5.022214   -2.660945  -1.402493   \n",
      "4  -0.980644  0.961793 -3.631267  0.289733    3.921001   -2.176788  -1.624669   \n",
      "\n",
      "   ndvi_slope  \n",
      "0    0.017136  \n",
      "1    0.021835  \n",
      "2    0.007236  \n",
      "3    0.010499  \n",
      "4   -0.036052  \n",
      "\n",
      "Feature Engineering for Training Data completed.\n",
      "Variable 'X_train_features' is now available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming X_train_scaled is defined from Step 2.\n",
    "# Also, ndvi_cols should be defined from Step 1.\n",
    "\n",
    "\n",
    "# --- Step 3: Feature Engineering for Training Data ---\n",
    "print(\"\\nStep 3: Feature Engineering for Training Data...\")\n",
    "X_train_features = pd.DataFrame() # DataFrame to hold the new engineered features\n",
    "\n",
    "# Calculate basic statistics across the 27 NDVI time points for each row\n",
    "X_train_features['ndvi_mean'] = X_train_scaled.mean(axis=1)\n",
    "X_train_features['ndvi_std'] = X_train_scaled.std(axis=1)\n",
    "X_train_features['ndvi_min'] = X_train_scaled.min(axis=1)\n",
    "X_train_features['ndvi_max'] = X_train_scaled.max(axis=1)\n",
    "X_train_features['ndvi_range'] = X_train_features['ndvi_max'] - X_train_features['ndvi_min']\n",
    "\n",
    "# Extract the first and last NDVI values directly\n",
    "X_train_features['ndvi_first'] = X_train_scaled.iloc[:, 0]\n",
    "X_train_features['ndvi_last'] = X_train_scaled.iloc[:, -1]\n",
    "\n",
    "# Calculate NDVI Trend (Slope) using a simple linear regression for each time series\n",
    "slopes = []\n",
    "# Create an array representing time points (0 to 26 for 27 NDVI values)\n",
    "time_points = np.arange(X_train_scaled.shape[1]).reshape(-1, 1)\n",
    "\n",
    "# Iterate through each row (each sample's NDVI time series)\n",
    "for index, row in X_train_scaled.iterrows():\n",
    "    model = LinearRegression() # Create a simple linear regression model\n",
    "    model.fit(time_points, row.values) # Fit model: NDVI values (y) vs. time point indices (x)\n",
    "    slopes.append(model.coef_[0]) # The coefficient (coef_[0]) is the slope of the line\n",
    "X_train_features['ndvi_slope'] = slopes\n",
    "\n",
    "print(\"Training data: New features created.\")\n",
    "print(f\"Engineered training features shape: {X_train_features.shape}\")\n",
    "print(X_train_features.head()) # Display the first few rows of engineered features\n",
    "\n",
    "print(\"\\nFeature Engineering for Training Data completed.\")\n",
    "print(\"Variable 'X_train_features' is now available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2935a-43f6-4897-ac2b-2c3a0e44075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Preprocessing & Feature Engineering for Test Data...\n",
      "Dropped 'Unnamed: 0' column from test_df.\n",
      "Test data: Missing values imputed using training imputer.\n",
      "Test data: Features scaled using training scaler.\n",
      "Test data: New features created.\n",
      "Engineered test features shape: (2845, 8)\n",
      "   ndvi_mean  ndvi_std  ndvi_min  ndvi_max  ndvi_range  ndvi_first  ndvi_last  \\\n",
      "0  -0.070332  0.955974 -1.782583  1.744096    3.526679    0.703785   1.725019   \n",
      "1   0.069367  0.866696 -1.660023  1.760748    3.420770    0.598592  -0.703466   \n",
      "2   0.102925  1.043168 -1.475840  2.072498    3.548338    0.684973  -0.707931   \n",
      "3  -0.061574  1.030381 -1.804253  1.939781    3.744034    0.545740   1.827161   \n",
      "4   0.381189  1.030266 -1.642597  2.272905    3.915503    0.727963  -0.496199   \n",
      "\n",
      "   ndvi_slope  \n",
      "0    0.008428  \n",
      "1    0.006632  \n",
      "2    0.005883  \n",
      "3    0.014902  \n",
      "4    0.004962  \n",
      "\n",
      "Preprocessing and Feature Engineering for Test Data completed.\n",
      "Variable 'X_test_features' is now available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming test_df, ndvi_cols, imputer, and scaler are already defined\n",
    "# from previous steps (Data Loading and Step 2).\n",
    "# If you are running this as a separate block, ensure these variables are available.\n",
    "\n",
    "\n",
    "# --- Step 4: Preprocessing and Feature Engineering for Test Data ---\n",
    "# Apply the SAME transformations (using the imputer and scaler fitted on training data)\n",
    "print(\"\\nStep 4: Preprocessing & Feature Engineering for Test Data...\")\n",
    "\n",
    "# Drop 'Unnamed: 0' column if it exists in test_df\n",
    "if \"Unnamed: 0\" in test_df.columns:\n",
    "    test_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    print(\"Dropped 'Unnamed: 0' column from test_df.\")\n",
    "\n",
    "\n",
    "# Separate features (X) for test set (using the same NDVI columns identified from training data)\n",
    "X_test_raw = test_df[ndvi_cols].copy()\n",
    "\n",
    "# Impute Missing Values in Test Data (using the 'imputer' fitted on training data)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test_raw), columns=ndvi_cols)\n",
    "print(\"Test data: Missing values imputed using training imputer.\")\n",
    "\n",
    "# Scale Features in Test Data (using the 'scaler' fitted on training data)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed), columns=ndvi_cols)\n",
    "print(\"Test data: Features scaled using training scaler.\")\n",
    "\n",
    "# Feature Engineering for Test Data (applying the exact same calculations as for training data)\n",
    "X_test_features = pd.DataFrame()\n",
    "X_test_features['ndvi_mean'] = X_test_scaled.mean(axis=1)\n",
    "X_test_features['ndvi_std'] = X_test_scaled.std(axis=1)\n",
    "X_test_features['ndvi_min'] = X_test_scaled.min(axis=1)\n",
    "X_test_features['ndvi_max'] = X_test_scaled.max(axis=1)\n",
    "X_test_features['ndvi_range'] = X_test_features['ndvi_max'] - X_test_features['ndvi_min']\n",
    "X_test_features['ndvi_first'] = X_test_scaled.iloc[:, 0]\n",
    "X_test_features['ndvi_last'] = X_test_scaled.iloc[:, -1]\n",
    "\n",
    "# Re-using the 'time_points' array created earlier for consistency\n",
    "# if not available, define: time_points = np.arange(X_test_scaled.shape[1]).reshape(-1, 1)\n",
    "test_slopes = []\n",
    "for index, row in X_test_scaled.iterrows():\n",
    "    model = LinearRegression()\n",
    "    model.fit(time_points, row.values)\n",
    "    test_slopes.append(model.coef_[0])\n",
    "X_test_features['ndvi_slope'] = test_slopes\n",
    "\n",
    "print(\"Test data: New features created.\")\n",
    "print(f\"Engineered test features shape: {X_test_features.shape}\")\n",
    "print(X_test_features.head())\n",
    "\n",
    "print(\"\\nPreprocessing and Feature Engineering for Test Data completed.\")\n",
    "print(\"Variable 'X_test_features' is now available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf2bdb-d6c8-4d22-b7a3-65fc0031788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Encoding Target Labels (for 'class' column)...\n",
      "Original classes: ['farm' 'forest' 'grass' 'impervious' 'orchard' 'water']\n",
      "Encoded labels for first 5 training samples: [5 5 5 5 5]\n",
      "\n",
      "Encoding of target labels completed.\n",
      "Variable 'y_train_encoded' is now available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming y_train_raw (your 'class' column from train_df) is already defined\n",
    "# from previous data loading and preprocessing steps.\n",
    "# If you are running this as a separate block, ensure y_train_raw is available.\n",
    "\n",
    "\n",
    "# --- Step 5: Encode Target Labels ---\n",
    "print(\"\\nStep 5: Encoding Target Labels (for 'class' column)...\")\n",
    "label_encoder = LabelEncoder() # Initialize a LabelEncoder\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_raw) # Fit and transform the string labels to numbers\n",
    "\n",
    "print(f\"Original classes: {label_encoder.classes_}\") # Show the mapping from numbers back to original class names\n",
    "print(f\"Encoded labels for first 5 training samples: {y_train_encoded[:5]}\") # Display first few encoded labels\n",
    "\n",
    "print(\"\\nEncoding of target labels completed.\")\n",
    "print(\"Variable 'y_train_encoded' is now available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3190ee-e803-4e40-ae11-d688ebb15ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Training Logistic Regression Model...\n",
      "Model training data shape: (6400, 8)\n",
      "Model validation data shape: (1600, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model trained successfully!\n",
      "\n",
      "Model training completed.\n",
      "Variables 'model', 'X_train_split', 'X_val', 'y_train_split', and 'y_val' are now available.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score # This will be used in the next step, but often imported here\n",
    "\n",
    "# Assuming X_train_features and y_train_encoded are already defined\n",
    "# from previous steps (Feature Engineering and Target Encoding).\n",
    "\n",
    "\n",
    "# --- Step 6: Train Logistic Regression Model ---\n",
    "print(\"\\nStep 6: Training Logistic Regression Model...\")\n",
    "# Split training data into a smaller training set and a validation set\n",
    "# This helps us evaluate model performance on data it hasn't seen during training\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_features, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded\n",
    ")\n",
    "\n",
    "print(f\"Model training data shape: {X_train_split.shape}\")\n",
    "print(f\"Model validation data shape: {X_val.shape}\")\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "# 'multi_class='multinomial'' is used for classification problems with more than two classes\n",
    "# 'solver='lbfgs'' is a good general-purpose solver for multinomial logistic regression\n",
    "# 'max_iter=1000' ensures the model has enough iterations to find the best fit\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "model.fit(X_train_split, y_train_split) # Train the model using the engineered features and encoded labels\n",
    "\n",
    "print(\"Logistic Regression model trained successfully!\")\n",
    "\n",
    "print(\"\\nModel training completed.\")\n",
    "print(\"Variables 'model', 'X_train_split', 'X_val', 'y_train_split', and 'y_val' are now available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc308443-c72c-499a-8701-16e0e7e300a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: Evaluating the Model on Validation Set...\n",
      "Validation Accuracy: 0.8481\n",
      "\n",
      "Model evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'model', 'X_val', and 'y_val' are already defined from Step 6.\n",
    "# If you are running this as a separate block, ensure these variables are available.\n",
    "\n",
    "\n",
    "# --- Step 7: Evaluate the Model ---\n",
    "print(\"\\nStep 7: Evaluating the Model on Validation Set...\")\n",
    "y_val_pred = model.predict(X_val) # Make predictions on the validation set\n",
    "accuracy = accuracy_score(y_val, y_val_pred) # Calculate the accuracy of predictions\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\") # Print the accuracy score\n",
    "\n",
    "print(\"\\nModel evaluation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ad42e-56d3-4953-b250-218606855004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in d:\\anaconda3\\lib\\site-packages (3.0.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7523c7-ffe5-41ea-a1fb-1ec7d44264fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall pandas\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression \u001b[38;5;66;03m# Needed for the model object\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression # Needed for the model object\n",
    "# Assuming 'model', 'X_test_features', 'label_encoder', and 'test_ids' are already defined\n",
    "# from previous steps (Model Training, Feature Engineering for Test Data, and Data Loading).\n",
    "# If you are running this as a separate block, ensure these variables are available.\n",
    "\n",
    "\n",
    "# --- Step 8: Predict on Test Set and Generate Submission File ---\n",
    "print(\"\\nStep 8: Predicting on Test Set and Generating Submission File...\")\n",
    "test_predictions_encoded = model.predict(X_test_features) # Make predictions on the engineered test data\n",
    "\n",
    "# Convert numerical predictions back to original class names (e.g., 0 -> 'farm', 1 -> 'forest')\n",
    "final_test_predictions = label_encoder.inverse_transform(test_predictions_encoded)\n",
    "\n",
    "# Create the submission DataFrame in the format required by Kaggle\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"class\": final_test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file to the current directory\n",
    "# index=False prevents pandas from writing the DataFrame index as a column in the CSV\n",
    "submission_df.to_csv(\"submission_output.csv\", index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully in the current directory!\")\n",
    "print(\"Here are the first few rows of your generated submission file:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "print(\"\\n--- All steps completed! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db78361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall pandas\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression \u001b[38;5;66;03m# Needed for the model object\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression # Needed for the model object\n",
    "# Assuming 'model', 'X_test_features', 'label_encoder', and 'test_ids' are already defined\n",
    "# from previous steps (Model Training, Feature Engineering for Test Data, and Data Loading).\n",
    "# If you are running this as a separate block, ensure these variables are available.\n",
    "\n",
    "\n",
    "# --- Step 8: Predict on Test Set and Generate Submission File ---\n",
    "print(\"\\nStep 8: Predicting on Test Set and Generating Submission File...\")\n",
    "test_predictions_encoded = model.predict(X_test_features) # Make predictions on the engineered test data\n",
    "\n",
    "# Convert numerical predictions back to original class names (e.g., 0 -> 'farm', 1 -> 'forest')\n",
    "final_test_predictions = label_encoder.inverse_transform(test_predictions_encoded)\n",
    "\n",
    "# Create the submission DataFrame in the format required by Kaggle\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"class\": final_test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file to the current directory\n",
    "# index=False prevents pandas from writing the DataFrame index as a column in the CSV\n",
    "submission_df.to_csv(\"submission_output.csv\", index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully in the current directory!\")\n",
    "print(\"Here are the first few rows of your generated submission file:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "print(\"\\n--- All steps completed! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
